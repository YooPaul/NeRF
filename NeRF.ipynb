{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeRF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1jYKvlV8V1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from math import sin, cos, sqrt, pi\n",
        "from scipy.spatial.transform import Rotation\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import scipy.interpolate as interpolate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbC3AvPGIAf-",
        "outputId": "1bd83800-a8e1-4501-8ffe-058d5dc29ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "vmylOrqfcZuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cameras = {}\n",
        "images = {}\n",
        "\n",
        "with open('south_building/cameras.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(3, len(lines)):\n",
        "        vals = lines[i].split(' ')\n",
        "\n",
        "        camera_id = int(vals[0])\n",
        "        intrinsics = {}\n",
        "\n",
        "        intrinsics['W'] = int(vals[2])\n",
        "        intrinsics['H'] = int(vals[3])\n",
        "\n",
        "        intrinsics['f'] = float(vals[4])\n",
        "        intrinsics['cx'] = int(vals[5])\n",
        "        intrinsics['cy'] = int(vals[6])\n",
        "\n",
        "        intrinsics['k'] = float(vals[7])\n",
        "\n",
        "        cameras[camera_id] = intrinsics\n",
        "\n",
        "with open('south_building/images.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(4, len(lines), 2):\n",
        "        vals = lines[i].split(' ')\n",
        "        image_name = vals[-1]\n",
        "        extrinsics = {}\n",
        "\n",
        "        qw = float(vals[1])\n",
        "        qx = float(vals[2])\n",
        "        qy = float(vals[3])\n",
        "        qz = float(vals[4])\n",
        "        R = Rotation.from_quat([qx, qy, qz, qw]).as_matrix()\n",
        "        \n",
        "        tx  = float(vals[5])\n",
        "        ty = float(vals[6])\n",
        "        tz = float(vals[7])\n",
        "        t = np.array([tx, ty, tz]).reshape((3,1))\n",
        "\n",
        "        extrinsics['R'] = R\n",
        "        extrinsics['t'] = t\n",
        "        extrinsics['c_id'] = int(vals[8])\n",
        "        \n",
        "        images[image_name] = extrinsics\n",
        "\n"
      ],
      "metadata": {
        "id": "y0zz-0_qIz_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataset = []\n",
        "basepath = '/content/drive/MyDrive/Colab Notebooks/south_building/'\n",
        "for i, image in enumerate(images):\n",
        "    fullpath = basepath + image[:-1] # get rid of \\n character\n",
        "    img = cv2.imread(fullpath) # H x W x c\n",
        "    extrinsics = images[image]\n",
        "    intrinsics = cameras[extrinsics['c_id']]\n",
        "\n",
        "    R = extrinsics['R']\n",
        "    t = extrinsics['t']\n",
        "    camera_pos = - R.T @ t\n",
        "\n",
        "    f = intrinsics['f']\n",
        "    cx = intrinsics['cx']\n",
        "    cy = intrinsics['cy']\n",
        "\n",
        "    # Radial distortion coefficient\n",
        "    k = intrinsics['k']\n",
        "\n",
        "    # From each image sample 70000 rays\n",
        "    for u in range(intrinsics['W']):\n",
        "        # Apply inverse intrinsic matrix\n",
        "        xpp = (u - cx) / f\n",
        "        for v in range(intrinsics['H']):\n",
        "            ypp = (v - cy) / f\n",
        "\n",
        "            # Radial distortion correction\n",
        "            #roots = np.roots([1, 2*k, 1, -(xpp**2 + ypp**2)])\n",
        "            #r_sq = roots.max().astype(float).item(0)\n",
        "\n",
        "            #assert(r_sq >= 0)\n",
        "\n",
        "            # Pixel (u,v) in 3D camera coordinate space\n",
        "            xp = xpp #/ (1 + k * r_sq)\n",
        "            yp = ypp #/ (1 + k * r_sq)\n",
        "            zp = 1.0\n",
        "\n",
        "            # Pixel (u,v) in 3D world coordinate space\n",
        "            x = R.T @ (np.array([xp, yp, zp]).reshape(3,1) - t)\n",
        "\n",
        "            # Ray direction\n",
        "            d = x - camera_pos\n",
        "            d = d / np.linalg.norm(d, axis=0) # Normalize\n",
        "            \n",
        "            color = img[v, u] # size 3\n",
        "            dataset.append((camera_pos, d, color))\n",
        "    break"
      ],
      "metadata": {
        "id": "RZRW--pIs_AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "U5JHYf1zbiXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeRFDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, transforms=None):\n",
        "        super(NeRFDataset, self).__init__()\n",
        "        \n",
        "        self.data = data\n",
        "        self.transforms = transforms\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of sequences in the dataset\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        #camera_pos, d, color = self.data[idx]\n",
        "        return self.data[idx] "
      ],
      "metadata": {
        "id": "xOWcs70jZOjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "bCBru77gZPbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, L):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.L = L\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc = torch.zeros(x.shape[0], 2 * self.L * x.shape[1])\n",
        "        for row in range(x.shape[0]):\n",
        "            for i in range(x.shape[1]):\n",
        "                p = x[row, i]\n",
        "                for new_i in range(self.L):\n",
        "                    x_enc[row, i * 2 * self.L + new_i] = sin(2**new_i * pi * p)\n",
        "                    x_enc[row, i * 2 * self.L + new_i + 1] = cos(2**new_i * pi * p)\n",
        "\n",
        "        return x_enc\n",
        "\n",
        "\n",
        "class NeRF(nn.Module):\n",
        "    def __init__(self, L1, L2, input_dim=3, layers=8, feature_dim=256):\n",
        "        super(NeRF, self).__init__()\n",
        "        \n",
        "        self.pos_enc1 = PositionalEncoding(L1)\n",
        "        self.pos_enc2 = PositionalEncoding(L2)\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(2 * L1 * input_dim, feature_dim))\n",
        "        for i in range(layers):\n",
        "            modules.append(nn.ReLU())\n",
        "            if i == layers - 1:\n",
        "                modules.append(nn.Linear(feature_dim, feature_dim + 1))\n",
        "            else:\n",
        "                modules.append(nn.Linear(feature_dim, feature_dim))\n",
        "            \n",
        "        self.MLP = nn.Sequential(*modules)\n",
        "\n",
        "        self.linear = nn.Linear(2 * L2 * input_dim + feature_dim, 128)\n",
        "        self.output = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d = self.pos_enc2(x[:,3:])\n",
        "        x = self.pos_enc1(x[:,:3])\n",
        "\n",
        "        output = self.MLP(x)\n",
        "        density, latent_code = output[:,:1], output[:,1:]\n",
        "\n",
        "        latent_code = F.relu(self.linear( torch.concat( (d, latent_code), dim=-1) ) )\n",
        "        color = self.output(latent_code)\n",
        "\n",
        "        return torch.relu(density), torch.sigmoid(color)"
      ],
      "metadata": {
        "id": "JK5ANlYXZRwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ZkrASa87ZiGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def integrate_color(N, density, color, ts, weights=None):\n",
        "    C = torch.tensor(0.0)\n",
        "    for i in range(N):\n",
        "        \n",
        "        T_i = torch.tensor(0.0)\n",
        "        for j in range(i):\n",
        "            delta_j = ts[j + 1] - ts[j]\n",
        "            sigma_j = density[j]\n",
        "            T_i = T_i + sigma_j * delta_j\n",
        "\n",
        "        T_i = torch.exp(-T_i)\n",
        "        sigma_i = density[i]\n",
        "        delta_i = ts[i + 1] - ts[i] if i != N - 1 else ts[i] - ts[i - 1]\n",
        "        c_i = color[i]\n",
        " \n",
        "        w_i = T_i * (1 - torch.exp(-sigma_i * delta_i))\n",
        "        if weights is not None:\n",
        "            weights.append(w_i.item())\n",
        "\n",
        "        C = C + w_i * c_i\n",
        "    \n",
        "    return C\n",
        "\n",
        "def inverse_transform_sampling(X, pdf, n):\n",
        "    samples = []\n",
        "    U = np.random.uniform(size=n)\n",
        "\n",
        "    for i in range(n):\n",
        "        u = U[i]\n",
        "        if u <= pdf[0]:\n",
        "            samples.append(np.random.uniform(tn, X[0]))\n",
        "        else:\n",
        "            for j in range(1, len(pdf)):\n",
        "                if(sum(pdf[0:j]) < u and u <= sum(pdf[0:j + 1])):\n",
        "                    samples.append(np.random.uniform(X[j - 1], X[j]))\n",
        "    return samples"
      ],
      "metadata": {
        "id": "o0xWtoWZQmxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "\n",
        "coarse_model = NeRF(10, 4)\n",
        "fine_model = NeRF(10, 4)\n",
        "optim = torch.optim.Adam(list(coarse_model.parameters()) + list(fine_model.parameters()), lr=0.01)\n",
        "coarse_model = coarse_model.to(device)\n",
        "fine_model = fine_model.to(device)\n",
        "\n",
        "# Points to sample\n",
        "Nc = 64\n",
        "Nf = 128\n",
        "tn = 1\n",
        "tf = 50\n",
        "for idx, ray in enumerate(dataset):\n",
        "    optim.zero_grad()\n",
        "    camera_pos, d, ground_truth_color = ray\n",
        "    ground_truth_color = torch.from_numpy(ground_truth_color).unsqueeze(0) / 255.0\n",
        "\n",
        "    # Stratified sampling\n",
        "    points = []\n",
        "    ts = []\n",
        "    for bin in range(Nc):\n",
        "        ti = np.random.uniform(tn + (bin - 1) / Nc * (tf - tn), tn + bin / Nc * (tf - tn))\n",
        "        points.append((camera_pos + ti * d).reshape(3).tolist() + d.reshape(3).tolist())\n",
        "        ts.append(ti)\n",
        "\n",
        "    points = torch.tensor(points) # Nc x 6\n",
        "    \n",
        "    density, color = coarse_model(points)\n",
        "\n",
        "    weights = []\n",
        "    Cc = integrate_color(Nc, density, color, ts, weights)\n",
        "\n",
        "    #density, color = fine_model(points)\n",
        "    #Cf = integrate_color(Nc, density, color, ts)\n",
        "\n",
        "    # Use weights as a PDF(Probability Density Function) to inverse transform sample Nf points\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / np.linalg.norm(weights)\n",
        "\n",
        "    new_ts = inverse_transform_sampling(ts, weights, Nf)\n",
        "\n",
        "    new_points = []\n",
        "    for new_t in new_ts:\n",
        "        new_points.append((camera_pos + new_t * d).reshape(3).tolist() + d.reshape(3).tolist())\n",
        "    \n",
        "    new_points = torch.tensor(new_points) # Nf x 6\n",
        "\n",
        "    density, color = fine_model(torch.concat((points, new_points), dim=0)) # concat size (Nc + Nf, 6)\n",
        "    Cf = integrate_color(Nc + Nf, density, color, sorted(ts + new_ts))\n",
        " \n",
        "    loss = F.mse_loss(Cc.unsqueeze(0), ground_truth_color) + F.mse_loss(Cf.unsqueeze(0), ground_truth_color)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "      print('Idx:', idx)\n",
        "      print('Loss:', loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "SfijjaDazjch",
        "outputId": "630f39f8-f39d-42e8-d598-65c699c0b51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n",
            "Idx: 0\n",
            "Loss: 1.3543294668197632\n",
            "Idx: 10\n",
            "Loss: 1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-905732b9a2f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mCc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintegrate_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#density, color = fine_model(points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-9a01b2d509e0>\u001b[0m in \u001b[0;36mintegrate_color\u001b[0;34m(N, density, color, ts, weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mdelta_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msigma_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mT_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_j\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mT_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mT_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "full_dataset = NeRFDataset(dataset)\n",
        "#print(full_dataset[0])\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "\n",
        "model = NeRF(10, 4)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "#t = transforms.Compose([transforms.ToPILImage(mode='F'), transforms.Resize(32), transforms.ToTensor()])\n",
        "t = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "#print(train_dataset[0])\n",
        "\n",
        "#num_workers = multiprocessing.cpu_count()\n",
        "#print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': 1, #num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, **kwargs)\n",
        "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, **kwargs)\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for idx, (enc_input, dec_input) in enumerate(train):\n",
        "    # enc_input, dec_input -> N x Seq\n",
        "    enc_input, dec_input = enc_input.to(device), dec_input.to(device)\n",
        "    #data = data.float()\n",
        "    #label = label.float()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    \n",
        "    output = model(enc_input, dec_input[:, :-1], enc_mask, dec_mask) #torch.cat((data, style.view(1,1,-1).expand(BATCH_SIZE,-1,-1)), 2))\n",
        "    \n",
        "    # Remove all the pad tokens as we don't want to penalize the model for not learning the paddings\n",
        "    ground_truth = dec_input[:,1:].reshape(-1)\n",
        "    output = output.view(-1, output.shape[-1]) # output -> (N * Seq) x Vocab_size \n",
        "    loss = F.cross_entropy(output[ground_truth != dec_dic['<pad>']], ground_truth[ground_truth != dec_dic['<pad>']])\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if idx % 10 == 0:\n",
        "      print('Epoch:', epoch)\n",
        "      print('Loss:', loss.item())\n",
        "      #print('Prediction:', output[0].squeeze())\n",
        "      #print('Truth:', label[0].squeeze())\n",
        "      #torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Music_Style_Transfer/models/' + 'lstm_generator_style.pt')\n",
        "\n",
        "\n",
        "print('Finished training model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6TcPfSgZks1",
        "outputId": "2050f069-a98b-4203-c4d0-bec2f4031d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gamma torch.Size([50]) True\n",
            "linear.weight torch.Size([50, 10]) True\n",
            "linear.bias torch.Size([50]) True\n",
            "norm.weight torch.Size([50]) True\n",
            "norm.bias torch.Size([50]) True\n"
          ]
        }
      ]
    }
  ]
}