{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeRF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1jYKvlV8V1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from math import sin, cos, sqrt, pi\n",
        "from scipy.spatial.transform import Rotation\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import scipy.interpolate as interpolate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbC3AvPGIAf-",
        "outputId": "1bd83800-a8e1-4501-8ffe-058d5dc29ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "vmylOrqfcZuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cameras = {}\n",
        "images = {}\n",
        "\n",
        "with open('south_building/cameras.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(3, len(lines)):\n",
        "        vals = lines[i].split(' ')\n",
        "\n",
        "        camera_id = int(vals[0])\n",
        "        intrinsics = {}\n",
        "\n",
        "        intrinsics['W'] = int(vals[2])\n",
        "        intrinsics['H'] = int(vals[3])\n",
        "\n",
        "        intrinsics['f'] = float(vals[4])\n",
        "        intrinsics['cx'] = int(vals[5])\n",
        "        intrinsics['cy'] = int(vals[6])\n",
        "\n",
        "        intrinsics['k'] = float(vals[7])\n",
        "\n",
        "        cameras[camera_id] = intrinsics\n",
        "\n",
        "with open('south_building/images.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(4, len(lines), 2):\n",
        "        vals = lines[i].split(' ')\n",
        "        image_name = vals[-1]\n",
        "        extrinsics = {}\n",
        "\n",
        "        qw = float(vals[1])\n",
        "        qx = float(vals[2])\n",
        "        qy = float(vals[3])\n",
        "        qz = float(vals[4])\n",
        "        R = Rotation.from_quat([qx, qy, qz, qw]).as_matrix()\n",
        "        \n",
        "        tx  = float(vals[5])\n",
        "        ty = float(vals[6])\n",
        "        tz = float(vals[7])\n",
        "        t = np.array([tx, ty, tz]).reshape((3,1))\n",
        "\n",
        "        extrinsics['R'] = R\n",
        "        extrinsics['t'] = t\n",
        "        extrinsics['c_id'] = int(vals[8])\n",
        "        \n",
        "        images[image_name] = extrinsics\n",
        "\n"
      ],
      "metadata": {
        "id": "y0zz-0_qIz_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODOs\n",
        "\n",
        "\n",
        "*   Need to convert points in world coordinate space to NDC space.\n",
        "\n"
      ],
      "metadata": {
        "id": "FcvumhrINxWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataset = []\n",
        "basepath = '/content/drive/MyDrive/Colab Notebooks/south_building/'\n",
        "for i, image in enumerate(images):\n",
        "    fullpath = basepath + image[:-1] # get rid of \\n character\n",
        "    img = cv2.imread(fullpath) # H x W x c\n",
        "    extrinsics = images[image]\n",
        "    intrinsics = cameras[extrinsics['c_id']]\n",
        "\n",
        "    R = extrinsics['R']\n",
        "    t = extrinsics['t']\n",
        "    camera_pos = - R.T @ t\n",
        "\n",
        "    f = intrinsics['f']\n",
        "    cx = intrinsics['cx']\n",
        "    cy = intrinsics['cy']\n",
        "\n",
        "    # Radial distortion coefficient\n",
        "    k = intrinsics['k']\n",
        "\n",
        "    # From each image sample 70000 rays\n",
        "    for u in range(intrinsics['W']):\n",
        "        # Apply inverse intrinsic matrix\n",
        "        xpp = (u - cx) / f\n",
        "        for v in range(intrinsics['H']):\n",
        "            ypp = (v - cy) / f\n",
        "\n",
        "            # Radial distortion correction\n",
        "            #roots = np.roots([1, 2*k, 1, -(xpp**2 + ypp**2)])\n",
        "            #r_sq = roots.max().astype(float).item(0)\n",
        "\n",
        "            #assert(r_sq >= 0)\n",
        "\n",
        "            # Pixel (u,v) in 3D camera coordinate space\n",
        "            xp = xpp #/ (1 + k * r_sq)\n",
        "            yp = ypp #/ (1 + k * r_sq)\n",
        "            zp = 1.0\n",
        "\n",
        "            # Pixel (u,v) in 3D world coordinate space\n",
        "            x = R.T @ (np.array([xp, yp, zp]).reshape(3,1) - t)\n",
        "\n",
        "            # Ray direction\n",
        "            d = x - camera_pos\n",
        "            d = d / np.linalg.norm(d, axis=0) # Normalize\n",
        "            \n",
        "            color = img[v, u] # size 3\n",
        "            dataset.append((camera_pos, d, color))\n",
        "    break"
      ],
      "metadata": {
        "id": "RZRW--pIs_AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "### TODOs\n",
        "\n",
        "\n",
        "\n",
        "*   In training NeRF, each data point defines a ray shooting out of a camera origin and the label being the corresponding pixel value of that ray. \n",
        "\n"
      ],
      "metadata": {
        "id": "U5JHYf1zbiXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeRFDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, transforms=None):\n",
        "        super(NeRFDataset, self).__init__()\n",
        "        \n",
        "        self.data = data\n",
        "        self.transforms = transforms\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of sequences in the dataset\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        #camera_pos, d, color = self.data[idx]\n",
        "        return self.data[idx] "
      ],
      "metadata": {
        "id": "xOWcs70jZOjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "bCBru77gZPbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, L):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.L = L\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc = torch.zeros(x.shape[0], 2 * self.L * x.shape[1])\n",
        "        for row in range(x.shape[0]):\n",
        "            for i in range(x.shape[1]):\n",
        "                p = x[row, i]\n",
        "                for new_i in range(self.L):\n",
        "                    x_enc[row, i * 2 * self.L + new_i] = sin(2**new_i * pi * p)\n",
        "                    x_enc[row, i * 2 * self.L + new_i + 1] = cos(2**new_i * pi * p)\n",
        "\n",
        "        return x_enc\n",
        "\n",
        "\n",
        "class NeRF(nn.Module):\n",
        "    def __init__(self, L1, L2, input_dim=3, layers=8, feature_dim=256):\n",
        "        super(NeRF, self).__init__()\n",
        "        \n",
        "        self.pos_enc1 = PositionalEncoding(L1)\n",
        "        self.pos_enc2 = PositionalEncoding(L2)\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(2 * L1 * input_dim, feature_dim))\n",
        "        for i in range(layers):\n",
        "            modules.append(nn.ReLU())\n",
        "            if i == layers - 1:\n",
        "                modules.append(nn.Linear(feature_dim, feature_dim + 1))\n",
        "            else:\n",
        "                modules.append(nn.Linear(feature_dim, feature_dim))\n",
        "            \n",
        "        self.MLP = nn.Sequential(*modules)\n",
        "\n",
        "        self.linear = nn.Linear(2 * L2 * input_dim + feature_dim, 128)\n",
        "        self.output = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d = self.pos_enc2(x[:,3:])\n",
        "        x = self.pos_enc1(x[:,:3])\n",
        "\n",
        "        output = self.MLP(x)\n",
        "        density, latent_code = output[:,:1], output[:,1:]\n",
        "\n",
        "        latent_code = F.relu(self.linear( torch.concat( (d, latent_code), dim=-1) ) )\n",
        "        color = self.output(latent_code)\n",
        "\n",
        "        return torch.relu(density), torch.sigmoid(color)"
      ],
      "metadata": {
        "id": "JK5ANlYXZRwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "### TODOs\n",
        "\n",
        "\n",
        "*   Currently only a single ray is being processed at a time. Sampling points along the ray and integrating color is also being executed sequentially. Need to modify code to process a batch of rays at a time and use tensors to parallelize the computations. This initial inefficient/slow version of the code was written to better understand the algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkrASa87ZiGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def integrate_color(N, density, color, ts, weights=None):\n",
        "    C = torch.tensor(0.0)\n",
        "    for i in range(N):\n",
        "        \n",
        "        T_i = torch.tensor(0.0)\n",
        "        for j in range(i):\n",
        "            delta_j = ts[j + 1] - ts[j]\n",
        "            sigma_j = density[j]\n",
        "            T_i = T_i + sigma_j * delta_j\n",
        "\n",
        "        T_i = torch.exp(-T_i)\n",
        "        sigma_i = density[i]\n",
        "        delta_i = ts[i + 1] - ts[i] if i != N - 1 else ts[i] - ts[i - 1]\n",
        "        c_i = color[i]\n",
        " \n",
        "        w_i = T_i * (1 - torch.exp(-sigma_i * delta_i))\n",
        "        if weights is not None:\n",
        "            weights.append(w_i.item())\n",
        "\n",
        "        C = C + w_i * c_i\n",
        "    \n",
        "    return C\n",
        "\n",
        "def inverse_transform_sampling(X, pdf, n):\n",
        "    samples = []\n",
        "    U = np.random.uniform(size=n)\n",
        "\n",
        "    for i in range(n):\n",
        "        u = U[i]\n",
        "        if u <= pdf[0]:\n",
        "            samples.append(np.random.uniform(tn, X[0]))\n",
        "        else:\n",
        "            for j in range(1, len(pdf)):\n",
        "                if(sum(pdf[0:j]) < u and u <= sum(pdf[0:j + 1])):\n",
        "                    samples.append(np.random.uniform(X[j - 1], X[j]))\n",
        "    return samples"
      ],
      "metadata": {
        "id": "o0xWtoWZQmxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "\n",
        "coarse_model = NeRF(10, 4)\n",
        "fine_model = NeRF(10, 4)\n",
        "optim = torch.optim.Adam(list(coarse_model.parameters()) + list(fine_model.parameters()), lr=0.01)\n",
        "coarse_model = coarse_model.to(device)\n",
        "fine_model = fine_model.to(device)\n",
        "\n",
        "# Points to sample\n",
        "Nc = 64\n",
        "Nf = 128\n",
        "tn = 1\n",
        "tf = 50\n",
        "for idx, ray in enumerate(dataset):\n",
        "    optim.zero_grad()\n",
        "    camera_pos, d, ground_truth_color = ray\n",
        "    ground_truth_color = torch.from_numpy(ground_truth_color).unsqueeze(0) / 255.0\n",
        "\n",
        "    # Stratified sampling\n",
        "    points = []\n",
        "    ts = []\n",
        "    for bin in range(Nc):\n",
        "        ti = np.random.uniform(tn + (bin - 1) / Nc * (tf - tn), tn + bin / Nc * (tf - tn))\n",
        "        points.append((camera_pos + ti * d).reshape(3).tolist() + d.reshape(3).tolist())\n",
        "        ts.append(ti)\n",
        "\n",
        "    points = torch.tensor(points) # Nc x 6\n",
        "    \n",
        "    density, color = coarse_model(points)\n",
        "\n",
        "    weights = []\n",
        "    Cc = integrate_color(Nc, density, color, ts, weights)\n",
        "\n",
        "    #density, color = fine_model(points)\n",
        "    #Cf = integrate_color(Nc, density, color, ts)\n",
        "\n",
        "    # Use weights as a PDF(Probability Density Function) to inverse transform sample Nf points\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / np.linalg.norm(weights)\n",
        "\n",
        "    new_ts = inverse_transform_sampling(ts, weights, Nf)\n",
        "\n",
        "    new_points = []\n",
        "    for new_t in new_ts:\n",
        "        new_points.append((camera_pos + new_t * d).reshape(3).tolist() + d.reshape(3).tolist())\n",
        "    \n",
        "    new_points = torch.tensor(new_points) # Nf x 6\n",
        "\n",
        "    density, color = fine_model(torch.concat((points, new_points), dim=0)) # concat size (Nc + Nf, 6)\n",
        "    Cf = integrate_color(Nc + Nf, density, color, sorted(ts + new_ts))\n",
        " \n",
        "    loss = F.mse_loss(Cc.unsqueeze(0), ground_truth_color) + F.mse_loss(Cf.unsqueeze(0), ground_truth_color)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "      print('Idx:', idx)\n",
        "      print('Loss:', loss.item())"
      ],
      "metadata": {
        "id": "SfijjaDazjch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}